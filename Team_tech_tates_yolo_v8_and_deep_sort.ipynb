{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c23349aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: 'yolov8_DeepSORT'\n",
      "C:\\Users\\ASUS\\Desktop\\AU\\Tracking-and-counting-Using-YOLOv8-and-DeepSORT-main\n"
     ]
    }
   ],
   "source": [
    "%cd yolov8_DeepSORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461e5750",
   "metadata": {},
   "source": [
    "# Testing YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ac57944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\ASUS\\Downloads\\download.jpeg: 448x640 15 persons, 1 bench, 5 backpacks, 133.3ms\n",
      "Speed: 0.0ms preprocess, 133.3ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24.0, 0.0, 0.0, 0.0, 0.0, 13.0, 24.0, 24.0, 24.0, 24.0]\n",
      "Class: person\n",
      "Class: person\n",
      "Class: person\n",
      "Class: person\n",
      "Class: person\n",
      "Class: person\n",
      "Class: person\n",
      "Class: person\n",
      "Class: person\n",
      "Class: person\n",
      "Class: person\n",
      "Class: backpack\n",
      "Class: person\n",
      "Class: person\n",
      "Class: person\n",
      "Class: person\n",
      "Class: bench\n",
      "Class: backpack\n",
      "Class: backpack\n",
      "Class: backpack\n",
      "Class: backpack\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import cv2\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image\n",
    "import colorsys\n",
    "import numpy as np\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n.pt\")  \n",
    "\n",
    "results = model(r\"C:\\Users\\ASUS\\Downloads\\download.jpeg\", save=True)\n",
    "\n",
    "\n",
    "\n",
    "class_names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "for result in results:\n",
    "    boxes = result.boxes  \n",
    "    probs = result.probs  \n",
    "    cls = boxes.cls.tolist()  \n",
    "    xyxy = boxes.xyxy\n",
    "    xywh = boxes.xywh  \n",
    "    conf = boxes.conf\n",
    "    print(cls)\n",
    "    for class_index in cls:\n",
    "        class_name = class_names[int(class_index)]\n",
    "        print(\"Class:\", class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9afac",
   "metadata": {},
   "source": [
    "# SINGLE VIDEO DETECTION AND TRACKING USING YOLOv8 AND DEEPSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "945f584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort.utils.parser import get_config\n",
    "from deep_sort.deep_sort import DeepSort\n",
    "from deep_sort.sort.tracker import Tracker\n",
    "\n",
    "deep_sort_weights = 'deep_sort/deep/checkpoint/ckpt.t7'\n",
    "tracker = DeepSort(model_path=deep_sort_weights, max_age=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "533ff5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 84.4ms\n",
      "Speed: 3.5ms preprocess, 84.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 98.9ms\n",
      "Speed: 16.1ms preprocess, 98.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 89.7ms\n",
      "Speed: 10.7ms preprocess, 89.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 102.5ms\n",
      "Speed: 12.6ms preprocess, 102.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 109.6ms\n",
      "Speed: 4.9ms preprocess, 109.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 111.0ms\n",
      "Speed: 0.0ms preprocess, 111.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 115.7ms\n",
      "Speed: 9.3ms preprocess, 115.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 100.3ms\n",
      "Speed: 15.8ms preprocess, 100.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 85.9ms\n",
      "Speed: 0.0ms preprocess, 85.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "\n",
    "\n",
    "video_path = r\"C:\\Users\\ASUS\\Downloads\\Untitled video - Made with Clipchamp (1).mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "frames = []\n",
    "\n",
    "unique_track_ids = set()\n",
    "i = 0\n",
    "counter, fps, elapsed = 0, 0, 0\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        \n",
    "        og_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = og_frame.copy()\n",
    "\n",
    "        model = YOLO(\"yolov8n.pt\")  \n",
    "\n",
    "        results = model(frame, device='cpu', classes=0, conf=0.8)\n",
    "\n",
    "        class_names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "        for result in results:\n",
    "            boxes = result.boxes  \n",
    "            probs = result.probs  \n",
    "            cls = boxes.cls.tolist()  \n",
    "            xyxy = boxes.xyxy\n",
    "            conf = boxes.conf\n",
    "            xywh = boxes.xywh  \n",
    "            for class_index in cls:\n",
    "                class_name = class_names[int(class_index)]\n",
    "                \n",
    "\n",
    "        pred_cls = np.array(cls)\n",
    "        conf = conf.detach().cpu().numpy()\n",
    "        xyxy = xyxy.detach().cpu().numpy()\n",
    "        bboxes_xywh = xywh\n",
    "        bboxes_xywh = xywh.cpu().numpy()\n",
    "        bboxes_xywh = np.array(bboxes_xywh, dtype=float)\n",
    "        \n",
    "        tracks = tracker.update(bboxes_xywh, conf, og_frame)\n",
    "        \n",
    "        for track in tracker.tracker.tracks:\n",
    "            track_id = track.track_id\n",
    "            hits = track.hits\n",
    "            x1, y1, x2, y2 = track.to_tlbr()  \n",
    "            w = x2 - x1  \n",
    "            h = y2 - y1  \n",
    "\n",
    "            \n",
    "            red_color = (0, 0, 255)  # (B, G, R)\n",
    "            blue_color = (255, 0, 0)  # (B, G, R)\n",
    "            green_color = (0, 255, 0)  # (B, G, R)\n",
    "\n",
    "            \n",
    "            color_id = track_id % 3\n",
    "            if color_id == 0:\n",
    "                color = red_color\n",
    "            elif color_id == 1:\n",
    "                color = blue_color\n",
    "            else:\n",
    "                color = green_color\n",
    "\n",
    "            cv2.rectangle(og_frame, (int(x1), int(y1)), (int(x1 + w), int(y1 + h)), color, 2)\n",
    "\n",
    "            text_color = (0, 0, 0) \n",
    "            cv2.putText(og_frame, f\"{class_name}-{track_id}\", (int(x1) + 10, int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1, cv2.LINE_AA)\n",
    "\n",
    "            \n",
    "            unique_track_ids.add(track_id)\n",
    "\n",
    "       \n",
    "        person_count = len(unique_track_ids)\n",
    "\n",
    "        \n",
    "        current_time = time.perf_counter()\n",
    "        elapsed = (current_time - start_time)\n",
    "        counter += 1\n",
    "        if elapsed > 1:\n",
    "            fps = counter / elapsed\n",
    "            counter = 0\n",
    "            start_time = current_time\n",
    "\n",
    "        \n",
    "        cv2.putText(og_frame, f\"Person Count: {person_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        \n",
    "        frames.append(og_frame)\n",
    "        cv2.imshow(\"Video\", og_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "             break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb820766",
   "metadata": {},
   "source": [
    "# TWO CAMERA PERSON DETECTION AND TRACKING USING DEEPSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fce03e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 101.7ms\n",
      "Speed: 4.4ms preprocess, 101.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 107.7ms\n",
      "Speed: 0.0ms preprocess, 107.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 100.2ms\n",
      "Speed: 3.8ms preprocess, 100.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 85.1ms\n",
      "Speed: 4.6ms preprocess, 85.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 116.8ms\n",
      "Speed: 0.0ms preprocess, 116.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 88.3ms\n",
      "Speed: 0.0ms preprocess, 88.3ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 89.3ms\n",
      "Speed: 8.5ms preprocess, 89.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 87.7ms\n",
      "Speed: 0.0ms preprocess, 87.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 100.2ms\n",
      "Speed: 7.4ms preprocess, 100.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 98.4ms\n",
      "Speed: 0.0ms preprocess, 98.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 98.9ms\n",
      "Speed: 2.6ms preprocess, 98.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 99.7ms\n",
      "Speed: 0.0ms preprocess, 99.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 106.8ms\n",
      "Speed: 15.6ms preprocess, 106.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 97.8ms\n",
      "Speed: 0.0ms preprocess, 97.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 114.2ms\n",
      "Speed: 0.0ms preprocess, 114.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 99.4ms\n",
      "Speed: 0.0ms preprocess, 99.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 104.0ms\n",
      "Speed: 7.3ms preprocess, 104.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 98.5ms\n",
      "Speed: 0.0ms preprocess, 98.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 114.6ms\n",
      "Speed: 0.0ms preprocess, 114.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 111.0ms\n",
      "Speed: 0.0ms preprocess, 111.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 106.2ms\n",
      "Speed: 15.6ms preprocess, 106.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 106.1ms\n",
      "Speed: 0.0ms preprocess, 106.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 90.7ms\n",
      "Speed: 5.3ms preprocess, 90.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 85.9ms\n",
      "Speed: 3.0ms preprocess, 85.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 104.1ms\n",
      "Speed: 15.6ms preprocess, 104.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 95.5ms\n",
      "Speed: 3.5ms preprocess, 95.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 106.6ms\n",
      "Speed: 0.0ms preprocess, 106.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 83.6ms\n",
      "Speed: 0.7ms preprocess, 83.6ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 121.7ms\n",
      "Speed: 0.0ms preprocess, 121.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 98.1ms\n",
      "Speed: 0.0ms preprocess, 98.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 79.5ms\n",
      "Speed: 10.2ms preprocess, 79.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 85.6ms\n",
      "Speed: 0.0ms preprocess, 85.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 113.0ms\n",
      "Speed: 0.0ms preprocess, 113.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 98.9ms\n",
      "Speed: 0.0ms preprocess, 98.9ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 110.2ms\n",
      "Speed: 0.0ms preprocess, 110.2ms inference, 15.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 101.0ms\n",
      "Speed: 0.0ms preprocess, 101.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 120.4ms\n",
      "Speed: 0.0ms preprocess, 120.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 89.2ms\n",
      "Speed: 0.0ms preprocess, 89.2ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 133.9ms\n",
      "Speed: 0.0ms preprocess, 133.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 92.5ms\n",
      "Speed: 0.0ms preprocess, 92.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 93.3ms\n",
      "Speed: 13.4ms preprocess, 93.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 88.3ms\n",
      "Speed: 2.5ms preprocess, 88.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 105.0ms\n",
      "Speed: 0.0ms preprocess, 105.0ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 96.0ms\n",
      "Speed: 4.0ms preprocess, 96.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 88.0ms\n",
      "Speed: 3.2ms preprocess, 88.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 87.5ms\n",
      "Speed: 0.0ms preprocess, 87.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 95.7ms\n",
      "Speed: 14.7ms preprocess, 95.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.0ms\n",
      "Speed: 2.0ms preprocess, 80.0ms inference, 8.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 104.0ms\n",
      "Speed: 0.0ms preprocess, 104.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 102.6ms\n",
      "Speed: 0.0ms preprocess, 102.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 114.4ms\n",
      "Speed: 0.0ms preprocess, 114.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 90.1ms\n",
      "Speed: 0.0ms preprocess, 90.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 100.9ms\n",
      "Speed: 0.0ms preprocess, 100.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 88.0ms\n",
      "Speed: 11.7ms preprocess, 88.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "from deep_sort.utils.parser import get_config\n",
    "from deep_sort.deep_sort import DeepSort\n",
    "from deep_sort.sort.tracker import Tracker\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Define the video paths\n",
    "video_path1 = r\"C:\\Users\\ASUS\\Downloads\\WhatsApp Video 2023-10-03 at 00.28.41_76315df1.mp4\"\n",
    "video_path2 = r\"C:\\Users\\ASUS\\Downloads\\WhatsApp Video 2023-10-03 at 00.28.42_4b733403.mp4\"\n",
    "\n",
    "\n",
    "deep_sort_weights = 'deep_sort/deep/checkpoint/ckpt.t7'\n",
    "tracker1 = DeepSort(model_path=deep_sort_weights, max_age=70)\n",
    "tracker2 = DeepSort(model_path=deep_sort_weights, max_age=70)\n",
    "\n",
    "# Create video capture objects\n",
    "cap1 = cv2.VideoCapture(video_path1)\n",
    "cap2 = cv2.VideoCapture(video_path2)\n",
    "\n",
    "# Define the frame buffers\n",
    "frames1 = []\n",
    "frames2 = []\n",
    "\n",
    "# Start the loop\n",
    "while cap1.isOpened() and cap2.isOpened():\n",
    "    # Read frames from the two cameras\n",
    "    ret1, frame1 = cap1.read()\n",
    "    ret2, frame2 = cap2.read()\n",
    "\n",
    "    # If a frame is not read from one of the cameras, break the loop\n",
    "    if not ret1 or not ret2:\n",
    "        break\n",
    "\n",
    "    og_frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2RGB)\n",
    "    frame1 = og_frame1.copy()\n",
    "    og_frame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)\n",
    "    frame2 = og_frame2.copy()\n",
    "\n",
    "    # Perform object detection and tracking on each camera\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "    # Object detection on camera 1\n",
    "    results1 = model(frame1, device='cpu', classes=0, conf=0.8)\n",
    "\n",
    "    # Object detection on camera 2\n",
    "    results2 = model(frame2, device='cpu', classes=0, conf=0.8)\n",
    "\n",
    "    class_names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "    # Tracking on camera 1\n",
    "    for result in results1:\n",
    "            boxes = result.boxes  \n",
    "            probs = result.probs  \n",
    "            cls = boxes.cls.tolist()  \n",
    "            xyxy = boxes.xyxy\n",
    "            conf = boxes.conf\n",
    "            xywh = boxes.xywh  \n",
    "            for class_index in cls:\n",
    "                class_name = class_names[int(class_index)]\n",
    "                \n",
    "\n",
    "    pred_cls = np.array(cls)\n",
    "    conf = conf.detach().cpu().numpy()\n",
    "    xyxy = xyxy.detach().cpu().numpy()\n",
    "    bboxes_xywh1 = xywh\n",
    "    bboxes_xywh1 = xywh.cpu().numpy()\n",
    "    bboxes_xywh1= np.array(bboxes_xywh1, dtype=float)\n",
    "     # Assuming 'results1' is a list with detection results\n",
    "     # Assuming 'results1' is a list with detection results\n",
    "    tracks1 = tracker1.update(bboxes_xywh1, conf, og_frame1)\n",
    "\n",
    "    # Tracking on camera 2\n",
    "    for result in results2:\n",
    "            boxes = result.boxes  \n",
    "            probs = result.probs  \n",
    "            cls = boxes.cls.tolist()  \n",
    "            xyxy = boxes.xyxy\n",
    "            conf = boxes.conf\n",
    "            xywh = boxes.xywh  \n",
    "            for class_index in cls:\n",
    "                class_name = class_names[int(class_index)]\n",
    "                \n",
    "\n",
    "    pred_cls = np.array(cls)\n",
    "    conf = conf.detach().cpu().numpy()\n",
    "    xyxy = xyxy.detach().cpu().numpy()\n",
    "    bboxes_xywh2 = xywh\n",
    "    bboxes_xywh2 = xywh.cpu().numpy()\n",
    "    bboxes_xywh2= np.array(bboxes_xywh2, dtype=float)\n",
    "     # Assuming 'results2' is a list with detection results\n",
    "    tracks2 = tracker2.update(bboxes_xywh2, conf, og_frame2)\n",
    "\n",
    "    # Merge the tracks from both cameras\n",
    "    #tracks = tracks1 + tracks2\n",
    "    #tracks=tracks1\n",
    "\n",
    "    # Draw the bounding boxes and track IDs on the frames\n",
    "    for track in tracker1.tracker.tracks:\n",
    "        track_id = track.track_id\n",
    "        hits = track.hits\n",
    "        x1, y1, x2, y2 = track.to_tlbr()  # Get bounding box coordinates in (x1, y1, x2, y2) format\n",
    "        w = x2 - x1  # Calculate width\n",
    "        h = y2 - y1  # Calculate height\n",
    "\n",
    "        # Set color values for red, blue, and green\n",
    "        red_color = (0, 0, 255)  # (B, G, R)\n",
    "        blue_color = (255, 0, 0)  # (B, G, R)\n",
    "        green_color = (0, 255, 0)  # (B, G, R)\n",
    "\n",
    "        # Determine color based on track_id\n",
    "        color = green_color\n",
    "\n",
    "        cv2.rectangle(og_frame1, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "        cv2.putText(og_frame1, f\"{track_id}\", (int(x1) + 10, int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, color, 1, cv2.LINE_AA)\n",
    "        #cv2.rectangle(og_frame2, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "        #cv2.putText(og_frame2, f\"{track_id}\", (int(x1) + 10, int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    #0.5, color, 1, cv2.LINE_AA)\n",
    "    #tracks=tracks2\n",
    "    for track in tracker2.tracker.tracks:\n",
    "        track_id = track.track_id\n",
    "        hits = track.hits\n",
    "        x1, y1, x2, y2 = track.to_tlbr()  # Get bounding box coordinates in (x1, y1, x2, y2) format\n",
    "        w = x2 - x1  # Calculate width\n",
    "        h = y2 - y1  # Calculate height\n",
    "\n",
    "        # Set color values for red, blue, and green\n",
    "        red_color = (0, 0, 255)  # (B, G, R)\n",
    "        blue_color = (255, 0, 0)  # (B, G, R)\n",
    "        green_color = (0, 255, 0)  # (B, G, R)\n",
    "\n",
    "        # Determine color based on track_id\n",
    "        color = green_color\n",
    "        cv2.rectangle(og_frame2, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "        cv2.putText(og_frame2, f\"{track_id}\", (int(x1) + 10, int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, color, 1, cv2.LINE_AA)\n",
    "\n",
    "    # Append the frame to the frame buffers\n",
    "    #frames1.append(frame1)\n",
    "    #frames2.append(frame2)\n",
    "\n",
    "    cv2.imshow(\"Video1\", og_frame1)\n",
    "    cv2.imshow(\"Video2\", og_frame2)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture objects\n",
    "cap1.release()\n",
    "cap2.release()\n",
    "\n",
    "# Destroy all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156d9154",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
